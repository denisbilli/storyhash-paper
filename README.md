# StoryHash: Lightweight Multi-Modal Fusion for Robust Video Retrieval

[![Paper](https://img.shields.io/badge/Paper-arXiv-b31b1b.svg)](https://arxiv.org/abs/XXXX.XXXXX)
[![Conference](https://img.shields.io/badge/CVPR-2026-blue.svg)]()
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

> **When Action Features Boost Confidence, Not Detection**

Official implementation of our paper on multi-modal video retrieval with systematic robustness evaluation and ablation studies.

---

## ğŸ“„ Paper

**Title**: Lightweight Multi-Modal Fusion for Robust Video Retrieval: When Action Features Boost Confidence, Not Detection

**Authors**: Denis Billi

**Abstract**: Video content retrieval systems must handle realistic perturbations like cropping, compression, and speed changes while maintaining semantic accuracy. We present **StoryHash**, a lightweight multi-modal fusion architecture combining visual (CLIP), temporal (action features), and structural (scene graph) embeddings into a 527-dimensional vector. Through systematic evaluation on 89 DAVIS videos with 14 transformation types (1,242 tests), we achieve **98.6% Recall@1** and **0.9748 average similarity**. Our ablation study reveals a surprising dichotomy: **action features boost confidence (+28% similarity) but minimally impact detection rate (+0.4% Recall@1)**. This finding validates efficiency-focused designs where lightweight temporal features (15D, 2.8% overhead) dramatically improve match confidence without requiring expensive temporal models. We identify a CLIP vertical flip bias (84.3% detection) and show rotation is the only transformation where action features aid detection (-2.3% degradation without).

**Paper PDF**: [main.pdf](paper/main.pdf) (single-column, 19 pages) | [main_two_column.pdf](paper/main_two_column.pdf) (two-column, 14 pages)

**arXiv**: https://arxiv.org/abs/XXXX.XXXXX *(coming soon)*

---

## ğŸ”‘ Key Findings

1. **Detection vs Confidence Dichotomy**: Action features are "confidence boosters" (+28% similarity) not "detection enablers" (+0.4% Recall@1)
2. **CLIP Vertical Flip Bias**: 84.3% detection on vertical flip vs 100% horizontal (16pp gap)
3. **Rotation Special Case**: Only transformation where action features aid detection (-2.3%)
4. **Efficiency Validation**: 15D action features (2.8% overhead) provide 28% confidence gain
5. **Robustness**: 98.6% Recall@1 across 14 realistic transformations (crop, rotate, brightness, compression, speed, flip, watermark, combined)

---

## ğŸ“Š Results

### Robustness Benchmark (14 Transformations)

| Configuration | Recall@1 | Recall@3 | Avg Similarity | Min Sim | Max Sim |
|--------------|----------|----------|----------------|---------|---------|
| **Fusion (CLIP + Action + Graph)** | **98.6%** | **99.7%** | **0.9748** | 0.8764 | 0.9957 |
| CLIP-only (512D) | 98.2% | 99.7% | 0.6996 | 0.3314 | 0.9881 |
| Action-only (15D) | 1.1% | 2.2% | 0.7480 | 0.2825 | 0.9954 |

### Per-Transformation Performance

| Transform | Fusion R@1 | CLIP R@1 | Action R@1 |
|-----------|------------|----------|------------|
| crop_10 | 100% | 100% | 0% |
| crop_20 | 100% | 100% | 0% |
| rotate_5 | 100% | 98.9% | 0% |
| rotate_10 | 98.9% | 96.6% | 0% |
| brightness_20 | 100% | 100% | 0% |
| brightness_40 | 100% | 100% | 0% |
| compress_50 | 100% | 100% | 0% |
| compress_25 | 98.9% | 98.9% | 0% |
| speed_0.9 | 100% | 100% | 0% |
| speed_1.1 | 100% | 100% | 0% |
| flip_h | 100% | 100% | 0% |
| **flip_v** | **84.3%** | **84.3%** | 0% |
| watermark | 98.9% | 98.9% | 0% |
| combined | 98.9% | 98.9% | 11.2% |

**Key Insight**: Action features improve confidence (0.97 vs 0.70 similarity) without improving detection rate, except for rotation (-2.3% without action).

---

## ğŸ—ï¸ Architecture

```
Input Video (89 DAVIS 2017 videos, 2-5s, 480p)
    â”‚
    â”œâ”€â†’ [CLIP ViT-B/32]          â†’ 512D visual features (97.2% contribution)
    â”‚   â””â”€ Median pooling @ 2fps
    â”‚
    â”œâ”€â†’ [SAM2 Tracking]           â†’ 15D action features (2.8% contribution)
    â”‚   â”œâ”€ Velocity (mean, std, max) [3D]
    â”‚   â”œâ”€ Acceleration (mean, std, max) [3D]
    â”‚   â”œâ”€ Direction entropy [1D]
    â”‚   â”œâ”€ Spatial spread (mean, std, max) [3D]
    â”‚   â””â”€ Interaction density (collisions, proximity, cluster) [5D]
    â”‚
    â””â”€â†’ [Scene Graph]             â†’ 8D structural features (<0.1% contribution)
        â”œâ”€ Spatial relations histogram [4D]
        â””â”€ Temporal relations histogram [4D]
        
    â†’ Concatenate â†’ L2 Normalize â†’ 527D Embedding
    â†’ FAISS HNSW (M=16, efConstruction=200)
    â†’ Query: <0.01ms median, 0.32ms P99
```

---

## ğŸš€ Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/denisbilli/storyhash-paper.git
cd storyhash-paper

# Create environment
conda create -n storyhash python=3.10
conda activate storyhash

# Install dependencies
pip install -r requirements.txt
```

### Extract Features

```bash
# Extract CLIP features
python scripts/extract_clip_features.py \
    --video_dir data/davis/JPEGImages/480p \
    --output embeddings/clip_features.npy

# Extract action features (requires SAM2 tracking)
python scripts/extract_action_features.py \
    --video_dir data/davis/JPEGImages/480p \
    --tracks_dir data/davis/sam2_tracks \
    --output embeddings/action_features.npy

# Extract scene graph features
python scripts/extract_graph_features.py \
    --video_dir data/davis/JPEGImages/480p \
    --output embeddings/graph_features.npy

# Concatenate and normalize
python scripts/fuse_embeddings.py \
    --clip embeddings/clip_features.npy \
    --action embeddings/action_features.npy \
    --graph embeddings/graph_features.npy \
    --output embeddings/fusion_527d.npy
```

### Build Index

```bash
# Create FAISS HNSW index
python scripts/build_index.py \
    --embeddings embeddings/fusion_527d.npy \
    --output indices/storyhash.index \
    --M 16 \
    --efConstruction 200
```

### Query

```bash
# Retrieve similar videos
python scripts/query.py \
    --index indices/storyhash.index \
    --query_video path/to/video.mp4 \
    --top_k 5
```

### Robustness Benchmark

```bash
# Apply 14 transformations and evaluate
python scripts/benchmark_robustness.py \
    --video_dir data/davis/JPEGImages/480p \
    --index indices/storyhash.index \
    --output results/robustness_benchmark.json

# Generate per-transformation table
python scripts/generate_results_table.py \
    --benchmark results/robustness_benchmark.json \
    --output paper/figures/table1_per_transform.tex
```

### Ablation Study

```bash
# Test CLIP-only (512D)
python scripts/ablation_clip_only.py \
    --embeddings embeddings/clip_features.npy \
    --output results/ablation_clip.json

# Test Action-only (15D)
python scripts/ablation_action_only.py \
    --embeddings embeddings/action_features.npy \
    --output results/ablation_action.json

# Compare configurations
python scripts/compare_ablations.py \
    --fusion results/robustness_benchmark.json \
    --clip results/ablation_clip.json \
    --action results/ablation_action.json \
    --output paper/figures/fig2_ablation_summary.pdf
```

---

## ğŸ“ Repository Structure

```
storyhash-paper/
â”œâ”€â”€ paper/                      # LaTeX sources and compiled PDF
â”‚   â”œâ”€â”€ main.tex               # Single-column (19 pages)
â”‚   â”œâ”€â”€ main_two_column.tex    # Two-column (14 pages)
â”‚   â”œâ”€â”€ main_body.tex          # Shared content sections
â”‚   â”œâ”€â”€ references.bib         # Bibliography (14 citations)
â”‚   â”œâ”€â”€ figures/               # Paper figures (5 PDFs + 1 table)
â”‚   â”œâ”€â”€ main.pdf              # Compiled single-column
â”‚   â””â”€â”€ main_two_column.pdf   # Compiled two-column
â”‚
â”œâ”€â”€ scripts/                    # Implementation code
â”‚   â”œâ”€â”€ extract_clip_features.py
â”‚   â”œâ”€â”€ extract_action_features.py
â”‚   â”œâ”€â”€ extract_graph_features.py
â”‚   â”œâ”€â”€ fuse_embeddings.py
â”‚   â”œâ”€â”€ build_index.py
â”‚   â”œâ”€â”€ query.py
â”‚   â”œâ”€â”€ benchmark_robustness.py
â”‚   â”œâ”€â”€ ablation_clip_only.py
â”‚   â”œâ”€â”€ ablation_action_only.py
â”‚   â”œâ”€â”€ generate_results_table.py
â”‚   â”œâ”€â”€ compare_ablations.py
â”‚   â””â”€â”€ generate_paper_figures.py  # Reproduce all figures
â”‚
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ LICENSE                     # MIT License
â””â”€â”€ README.md                   # This file
```

---

## ğŸ¯ Reproduce Paper Results

All experiments in the paper can be reproduced using the provided scripts:

```bash
# 1. Download DAVIS 2017 dataset
wget https://data.vision.ee.ethz.ch/csergi/share/davis/DAVIS-2017-trainval-480p.zip
unzip DAVIS-2017-trainval-480p.zip -d data/davis

# 2. Extract all features
bash scripts/extract_all_features.sh

# 3. Run robustness benchmark (Table 1)
python scripts/benchmark_robustness.py

# 4. Run ablation studies (Figure 2, 3)
bash scripts/run_ablations.sh

# 5. Generate all figures
python scripts/generate_paper_figures.py

# 6. Compile paper
cd paper && pdflatex main.tex && bibtex main && pdflatex main.tex && pdflatex main.tex
```

**Expected Runtime**: ~4 hours on M2 Pro (feature extraction dominates)

---

## ğŸ“¦ Dependencies

- Python 3.10+
- PyTorch 2.1.0+
- transformers 4.35.0+ (CLIP)
- segment-anything-2 (SAM2 tracking)
- faiss-cpu 1.7.4+ (indexing)
- opencv-python 4.8.0+ (video processing)
- ffmpeg (transformations)

Full list in `requirements.txt`.

---

## ğŸ“ Citation

If you use this code or findings in your research, please cite:

```bibtex
@article{billi2025storyhash,
  title={Lightweight Multi-Modal Fusion for Robust Video Retrieval: When Action Features Boost Confidence, Not Detection},
  author={Billi, Denis},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2025}
}
```

---

## ğŸ™ Acknowledgments

We thank the open-source community for:
- [CLIP](https://github.com/openai/CLIP) (OpenAI)
- [SAM2](https://github.com/facebookresearch/segment-anything-2) (Meta AI)
- [FAISS](https://github.com/facebookresearch/faiss) (Meta AI)
- [DAVIS 2017](https://davischallenge.org/) (ETH Zurich)

This work was conducted independently without external funding.

---

## ğŸ“§ Contact

**Denis Billi**  
Email: denis@denisbilli.it  
GitHub: [@denisbilli](https://github.com/denisbilli)

---

## ğŸ“œ License

MIT License - see [LICENSE](LICENSE) file for details.

---

## ğŸ”— Links

- **Paper PDF**: [main.pdf](paper/main.pdf)
- **arXiv**: https://arxiv.org/abs/XXXX.XXXXX *(coming soon)*
- **Project Page**: https://github.com/denisbilli/storyhash-paper
- **Main Development Repo**: https://github.com/denisbilli/StoryHash *(private)*
