% Main body of the StoryHash paper (included by main.tex and main_two_column.tex)
% This file contains Method, Experiments, Results, Discussion, Limitations

% ============================================================================
% 3. METHOD
% ============================================================================
\section{Method}

\subsection{Architecture Overview}

\textbf{StoryHash} extracts a 527-dimensional embedding by fusing three components:

\begin{equation}
\text{StoryHash} = [\text{CLIP}_{\text{512D}}, \text{Action}_{\text{15D}}, \text{Graph}_{\text{8D}}]
\end{equation}

Figure~\ref{fig:pipeline} illustrates the complete pipeline from video input to retrieval results.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig1_pipeline_placeholder.pdf}
    \caption{StoryHash pipeline architecture. Input videos are sampled at 2 fps, features are extracted from three modalities (CLIP visual, SAM2 action, scene graph), concatenated into a 527D vector, and indexed using FAISS HNSW for efficient retrieval.}
    \label{fig:pipeline}
\end{figure}

\subsubsection{Visual Features (CLIP)}

We use \texttt{openai/clip-vit-base-patch32}~\cite{radford2021learning} to extract 512-dimensional embeddings. Given a video $V$ with $N$ frames sampled uniformly at 2 fps, we compute:

\begin{equation}
\mathbf{v}_{\text{CLIP}} = \text{median}(\{\text{CLIP}(f_i)\}_{i=1}^{N})
\end{equation}

where $\text{CLIP}(f_i) \in \mathbb{R}^{512}$ is the visual embedding of frame $f_i$. We use \textbf{median pooling} instead of mean to be robust to outlier frames (e.g., scene transitions, compression artifacts). The embedding is L2-normalized: $\|\mathbf{v}_{\text{CLIP}}\|_2 = 1$.

\subsubsection{Action Features (Temporal)}

We extract 15-dimensional motion features from SAM2~\cite{ravi2023sam2} object tracking trajectories. Given $M$ objects tracked across $T$ frames, we compute aggregate statistics:

\begin{equation}
\mathbf{v}_{\text{action}} = [\mathbf{v}_{\text{vel}}, \mathbf{v}_{\text{acc}}, \mathbf{v}_{\text{dir}}, \mathbf{v}_{\text{spread}}, \mathbf{v}_{\text{interact}}]
\end{equation}

where each component aggregates object-level measurements:

\noindent\textbf{Velocity features} (3D): For each object $m$, compute per-frame velocity $v_m(t) = \|\mathbf{p}_m(t) - \mathbf{p}_m(t-1)\|_2$ where $\mathbf{p}_m(t)$ is the centroid position. Aggregate: $\mu(v_m), \sigma(v_m), \max(v_m)$.

\noindent\textbf{Acceleration features} (2D): Compute $a_m(t) = |v_m(t) - v_m(t-1)|$. Aggregate: $\mu(a_m), \sigma(a_m)$.

\noindent\textbf{Direction consistency} (2D): Measure angular deviation $\theta_m(t)$ between consecutive motion vectors. Aggregate: $\mu(\cos\theta_m), \sigma(\cos\theta_m)$.

\noindent\textbf{Spatial spread} (3D): Compute pairwise distances $d_{ij}(t) = \|\mathbf{p}_i(t) - \mathbf{p}_j(t)\|_2$ for all object pairs. Aggregate: $\mu(d_{ij}), \sigma(d_{ij}), \max(d_{ij})$.

\noindent\textbf{Interaction density} (5D): Count object co-occurrences in spatial proximity (threshold: 50px). Features: mean count, std, max, edge ratio (fraction of frames with interactions), temporal coherence (duration of stable interactions).

All features are min-max normalized to $[0,1]$ and concatenated into $\mathbf{v}_{\text{action}} \in \mathbb{R}^{15}$.

\subsubsection{Scene Graph Features (Structural)}

We extract an 8-dimensional encoding of spatial and temporal relationships between objects using scene graph construction. Given tracked objects from SAM2, we build a dynamic graph where nodes represent objects and edges represent relationships. Spatial relations are defined by bounding box proximity: \texttt{near(obj1, obj2)} holds when the Euclidean distance between centroids is below a threshold (100 pixels). Temporal relations capture co-occurrence patterns: \texttt{co\_occurs(obj1, obj2)} indicates simultaneous presence in frames, while \texttt{follows(obj1, obj2)} represents temporal ordering in appearance.

The final 8D vector aggregates these relations through a normalized histogram of relation types across all frames, capturing structural patterns of object interactions. However, our experiments reveal that graph features contribute minimally ($<0.1\%$ performance impact) in the current formulation, likely due to the short video duration (2-5 seconds) and simple aggregation strategy. We therefore focus our analysis on CLIP + action fusion, reserving graph refinement with Graph Neural Networks for future work.

\subsection{Feature Fusion and Normalization}

The final StoryHash vector concatenates all three modalities:

\begin{equation}
\mathbf{h} = [\mathbf{v}_{\text{CLIP}}, \mathbf{v}_{\text{action}}, \mathbf{v}_{\text{graph}}] \in \mathbb{R}^{527}
\end{equation}

We L2-normalize the full 527D vector: $\mathbf{h} \leftarrow \mathbf{h} / \|\mathbf{h}\|_2$. This enables efficient cosine similarity via inner product:

\begin{equation}
\text{sim}(\mathbf{h}_1, \mathbf{h}_2) = \mathbf{h}_1^T \mathbf{h}_2 = \cos(\mathbf{h}_1, \mathbf{h}_2)
\end{equation}

Retrieval ranks videos by descending similarity to the query embedding.

\subsection{Indexing and Retrieval}

We use FAISS~\cite{douze2024faiss} \texttt{IndexHNSWFlat} (M=16, efConstruction=200) for efficient similarity search with inner product. HNSW (Hierarchical Navigable Small World) builds a multi-layer graph for approximate nearest neighbor search with sub-linear complexity. At our scale (89 videos), retrieval is exact with query latency $<0.01$ms median, 0.32ms P99.

\section{Experimental Setup}

\subsection{Dataset}

We use \textbf{DAVIS 2017}~\cite{perazzi2016benchmark} (Densely Annotated VIdeo Segmentation), a benchmark originally designed for video object segmentation that provides diverse natural scenes ideal for robustness evaluation. The dataset contains 90 videos, of which 89 are indexed in our retrieval system (one metadata file excluded). Content spans multiple categories including animals (dog, goat, camel), vehicles (bmx, motocross, car-shadow, train), sports activities (horsejump, kite-surf, parkour), and human interactions (scooter, walking, dancing), providing semantic diversity.

Each video ranges from 2 to 5 seconds in duration with a median of 76 frames at 24 fps, captured at 480p resolution (854×480 pixels). This temporal scale aligns with social media content (TikTok clips, Instagram stories) and enables comprehensive per-frame analysis without excessive computational cost. The presence of significant motion, occlusion, and natural lighting variations makes DAVIS suitable for evaluating retrieval robustness rather than clean-dataset performance. Its modest size (89 videos) enables exhaustive robustness evaluation—testing all 14 transformation types on every video yields 1,242 tests—while maintaining full reproducibility and tractability for ablation studies.

\subsection{Implementation Details}

\noindent\textbf{Hardware}: Apple M2 Pro (12-core CPU, 19-core GPU, 16GB RAM)

\noindent\textbf{Software}: Python 3.11, PyTorch 2.1.0, OpenCV 4.8.1, FAISS 1.7.4, transformers 4.35.0

\noindent\textbf{CLIP Model}: \texttt{openai/clip-vit-base-patch32} (vision encoder: ViT-B/32, 86M parameters)

\noindent\textbf{Sampling Rate}: 2 fps (extracted from 24 fps source via uniform sampling)

\noindent\textbf{Average Frames per Video}: 76 frames → 5 sampled frames (median)

\subsection{Transformations}

We generate 14 realistic perturbation types using \texttt{ffmpeg} to simulate social media re-sharing scenarios that video retrieval systems encounter in practice. \textbf{Spatial transformations} include centered cropping at two intensities (10\% and 20\% reduction implemented via \texttt{crop=0.9*iw:0.9*ih}), simulating mobile device framing or screenshot capture. \textbf{Rotation} applies small angular perturbations (5\degree and 10\degree counter-clockwise using \texttt{rotate=5*PI/180}), common in mobile uploads where device orientation is not locked.

\textbf{Photometric changes} adjust brightness by +20\% and +40\% (\texttt{eq=brightness=0.2}), representing display calibration differences or intentional color grading. \textbf{Compression} degrades quality through H.264 encoding at 50\% and 25\% bitrate (CRF=28 and CRF=35), modeling aggressive platform compression on Facebook, Twitter, or WhatsApp. \textbf{Temporal manipulation} alters playback speed by 0.9$\times$ (slower) and 1.1$\times$ (faster) using \texttt{setpts=1.1*PTS}, simulating TikTok/Reels speed adjustments for music synchronization.

\textbf{Geometric flips} include horizontal (\texttt{hflip}) and vertical (\texttt{vflip}) mirroring, where horizontal flips are common for aesthetic reasons ("selfie mode") while vertical flips are rare but reveal model biases. \textbf{Watermarking} overlays the text "SAMPLE" at the bottom-right corner via \texttt{drawtext}, representing channel branding or copyright notices on re-uploaded content. Finally, a \textbf{combined transformation} chains crop 10\% + flip\_h + speed 0.9$\times$ sequentially, simulating multi-step processing through messaging apps (e.g., WhatsApp forward with automatic cropping and compression).

Applying all 14 transformations to 89 videos yields 1,246 potential tests. However, two videos (\texttt{horsejump-high} and \texttt{parkour}) encountered aspect ratio edge cases during rotation that caused \texttt{ffmpeg} to fail, resulting in 1,242 valid tests. This transformation suite reflects realistic content modifications observed in social media ecosystems: mobile uploads introduce rotation and crop; platform compression reduces bitrate; content reuse adds watermarks; viral sharing applies flips for humor; and automated processing adjusts speed for engagement (TikTok 1.5$\times$ playback, slow-motion effects).

\subsection{Evaluation Metrics}

We evaluate retrieval performance using three complementary metrics. \textbf{Recall@k} measures the fraction of transformed videos where the original (untransformed) video appears within the top-k retrieval results. This metric captures detection capability: Recall@1 indicates whether the system correctly identifies the exact match as the top result, while Recall@3 allows for near-miss scenarios where the correct video ranks second or third. High Recall@k is critical for copyright systems where false negatives (missed matches) enable evasion.

\textbf{Average Similarity} computes the mean cosine similarity between the original video embedding and its transformed variant across all test cases. This metric quantifies match confidence: high similarity (close to 1.0) indicates the system is certain about the match, while lower similarity suggests uncertainty even when detection succeeds. For content moderation applications, similarity scores enable prioritization of borderline cases for human review.

\textbf{Min/Max Similarity} captures the range of similarity scores within each transformation type, revealing robustness variability. A tight range (high minimum, consistent maximum) indicates uniform performance across videos, while wide ranges suggest some videos are more robust than others to specific perturbations. These extreme values help identify failure modes and guide future improvements.

\subsection{Ablation Study Design}

To quantify the contribution of each modality to retrieval performance, we conduct a systematic ablation study comparing three configurations on identical sets of transformed videos. The \textbf{Fusion} configuration serves as our baseline, combining CLIP visual features (512D), action temporal features (15D), and scene graph structural features (8D) into the full 527D StoryHash embedding as described in Section 3.1.

The \textbf{CLIP-only} configuration isolates visual information by extracting only the 512D CLIP embeddings and zero-padding them to 527D to maintain FAISS index compatibility. This padding ensures fair comparison: all configurations use the same index structure and similarity computation (L2-normalized inner product). The CLIP-only mode answers the question: "How much can visual semantics alone achieve under realistic perturbations?"

The \textbf{Action-only} configuration tests whether motion features can retrieve videos independently of visual content. We extract only the 15D action features, zero-pad to 527D, and normalize. While we expect this configuration to perform poorly (temporal patterns lack semantic discrimination), it serves two purposes: (1) establishing a lower bound for temporal-only retrieval, and (2) identifying transformations where motion is surprisingly informative (e.g., speed changes). All three configurations run on the same 1,242 transformed test videos, enabling direct per-transformation comparison of Recall@k and similarity scores.

\section{Results}

\subsection{Robustness Benchmark (Fusion Baseline)}

The full StoryHash system (527D fusion) achieves strong robustness across all transformation types. \textbf{Recall@1} reaches 98.6\%, correctly retrieving the original video as the top result in 1,225 out of 1,242 tests. This high detection rate demonstrates that the multi-modal embedding remains discriminative even under aggressive perturbations like 75\% compression or combined transformations. \textbf{Recall@3} improves to 99.7\% (1,239/1,242), indicating that nearly all failures are "soft" misses where the correct video ranks second or third rather than being completely lost.

\textbf{Average Similarity} of 0.9748 reflects extremely high match confidence: transformed videos are nearly identical to their originals in embedding space (cosine similarity close to 1.0). This high confidence is critical for automated systems that require certainty before taking action (e.g., DMCA takedowns, content flagging). The similarity metric reveals that StoryHash not only detects matches but does so with conviction, minimizing borderline cases requiring human review.

Table~\ref{tab:per_transform} provides detailed per-transformation results. A critical observation emerges: vertical flip (flip\_v) is a significant outlier with only 84.3\% Recall@1, representing 14 failures out of 89 videos—substantially worse than the 100\% achieved by horizontal flip. This 16-percentage-point gap suggests a systematic orientation bias in CLIP's training data, which we investigate further in the failure case analysis (Section 5).

% Insert per-transformation table
\input{figures/table1_per_transform.tex}

\subsection{Ablation Study Results}

Figure~\ref{fig:ablation} summarizes the 3-way ablation study showing detection (Recall@1) and confidence (similarity) performance.

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.48\columnwidth}
        \includegraphics[width=\textwidth]{figures/fig2_ablation_summary.pdf}
        \caption{Detection (Recall@1)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\columnwidth}
        \includegraphics[width=\textwidth]{figures/fig3_similarity_delta.pdf}
        \caption{Confidence (Similarity)}
    \end{subfigure}
    \caption{3-way ablation study: (a) CLIP dominates detection (98.2\%), action adds +0.4\%; (b) Action dramatically boosts confidence (+28\% similarity).}
    \label{fig:ablation}
\end{figure}

\subsubsection{CLIP-only Performance}

Visual features alone achieve remarkable robustness: \textbf{Recall@1} of 98.2\% (1,224/1,246 tests) demonstrates that semantic visual understanding via CLIP is nearly sufficient for detection across most realistic perturbations. This represents only a -0.4\% drop compared to the full fusion system, indicating CLIP dominates detection capability. \textbf{Recall@3} of 99.4\% (1,239/1,246) shows that soft misses remain rare even without temporal information.

However, \textbf{average similarity} drops dramatically to 0.6996—a 28\% reduction compared to fusion (0.9748). This reveals a critical insight: CLIP correctly ranks videos but with lower confidence. Matches that fusion scores at 0.97-0.99 similarity (near-certain) fall to 0.65-0.75 with CLIP alone (plausible but uncertain). For automated systems requiring high-confidence decisions, this similarity gap is problematic: borderline scores necessitate human review, reducing throughput in content moderation pipelines.

\subsubsection{Action-only Performance}

Motion features in isolation prove inadequate for video retrieval: \textbf{Recall@1} of 1.1\% (14/1,246 tests) and \textbf{Recall@3} of 3.4\% (42/1,246) demonstrate that temporal patterns lack the discriminative power needed to identify specific videos. Only a single video (\texttt{schoolgirls}) was consistently detected across transformations, likely due to distinctive motion signatures (multiple people moving in coordinated patterns).

The reported \textbf{average similarity} of 0.7500 requires careful interpretation: this value is an artifact of zero-padding the 15D action vector to 527D followed by L2-normalization and cosine similarity computation. The high dimensionality of the padded zeros creates a baseline similarity floor that does not reflect meaningful semantic structure or discriminative power. Essentially, most videos score similarly because their 15D signals are insufficient to separate 89 distinct videos in 527D space. This finding validates our design choice: action features are complementary enhancements to visual features, not standalone retrieval solutions.

\subsubsection{Three-Way Comparison}

\begin{table}[h]
\centering
\caption{Overall ablation study results comparing Fusion, CLIP-only, and Action-only.}
\label{tab:ablation}
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{Fusion} & \textbf{CLIP-only} & \textbf{Action-only} \\
\midrule
Recall@1 (\%) & 98.6 & 98.2 (-0.4\%) & 1.1 (-97.5\%) \\
Avg Similarity & 0.9748 & 0.6996 (-28\%) & 0.7500 (-23\%) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Transformation-Specific Insights}

\subsubsection{Rotation: Only Case Where Action Helps Detection}

Spatial rotation disrupts CLIP's visual embeddings, but temporal motion patterns (velocity, direction) remain invariant. Action features compensate slightly: Fusion 98.9\% $\rightarrow$ CLIP-only 96.6\% (\textbf{-2.3\%}).

\subsubsection{flip\_v: CLIP Vertical Orientation Bias}

Figure~\ref{fig:flip_bias} shows the stark contrast between horizontal and vertical flip performance, confirming CLIP's training data contains mostly upright images.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\columnwidth]{figures/fig4_flip_bias.pdf}
    \caption{CLIP vertical flip bias: horizontal flip achieves 100\% detection while vertical flip drops to 84.3\% for both Fusion and CLIP-only (action features cannot compensate).}
    \label{fig:flip_bias}
\end{figure}

\subsubsection{Visual Transforms: CLIP Dominance}

All spatial (crop), photometric (brightness, compression), and watermark transforms achieve 100\% Recall@1 for both Fusion and CLIP-only. Action features contribute \textbf{only to confidence} (similarity), not detection.

\section{Failure Case Analysis}

We analyze specific failure modes to understand system limitations and modality contributions.

\subsection{Vertical Flip Failures}

14 videos failed Recall@1 under flip\_v, with similarity scores 0.86-0.92 (high but insufficient). Examples:

\begin{itemize}
    \item \textbf{scooter-black} (flip\_v): Rank 2, similarity 0.8861. Original shows person on scooter from rear view. Vertical flip inverts scene, causing CLIP to mismatch due to unnatural "upside-down person" configuration.
    \item \textbf{walking} (flip\_v): Rank 2, similarity 0.8911. Pedestrians walking normally become "walking on ceiling" after inversion, violating CLIP's learned gravity priors.
    \item \textbf{libby} (flip\_v): Rank 3+, similarity 0.8575 (lowest). Complex scene with dog and person; vertical flip creates physically implausible configuration.
\end{itemize}

\textbf{Analysis}: CLIP's vision encoder (ViT-B/32) learns positional encodings that encode "up" vs "down" semantic information. Training data bias toward upright images prevents generalization to inverted scenes. Action features (velocity, direction) are computed in image coordinates and thus also affected by flip\_v, explaining why fusion does not rescue CLIP failures.

\textbf{Comparison with flip\_h}: Horizontal flips preserve vertical orientation (gravity axis), allowing CLIP to recognize mirrored objects (e.g., car facing left vs right) as semantically identical. This asymmetry confirms orientation-specific bias.

\subsection{Rotation Edge Cases}

2 videos failed rotation due to aspect ratio issues (\texttt{horsejump-high}, \texttt{parkour}), excluded from analysis. Among successful rotations:

\begin{itemize}
    \item \textbf{Best case} (rotate\_10): \texttt{motocross-jump} achieves 0.9604 similarity (Rank 1). High-contrast subject (bike in air) remains discriminative despite 10° tilt.
    \item \textbf{Worst case} (rotate\_10): \texttt{drift-chicane} drops to 0.8945 similarity (Rank 1 by narrow margin). Dense scene with multiple cars and motion blur; rotation compounds visual complexity.
\end{itemize}

\textbf{Action features help here}: Fusion (98.9\%) outperforms CLIP-only (96.6\%) by -2.3\%. Motion vectors (velocity, direction) are computed in pixel space and remain consistent under small rotations, providing a secondary matching signal when CLIP struggles.

\subsection{Compression vs Watermark Robustness}

\textbf{compress\_25} (75\% bitrate reduction, CRF=35): 98.9\% R@1, 0.9525 avg similarity. 1 failure: \texttt{india} (complex cultural scene with crowd, bright colors). Aggressive compression artifacts (blocking, ringing) degrade CLIP visual features, but median pooling across frames provides some resilience.

\textbf{watermark}: 100\% R@1, 0.9972 avg similarity (most robust transform!). Text overlay "SAMPLE" at bottom-right corner occupies <5\% of frame area. CLIP's global average pooling (ViT) naturally downweights localized perturbations, focusing on semantic content. This validates StoryHash for \textbf{copyright detection against re-uploaded content with channel branding}.

\subsection{Speed Invariance Success}

\textbf{speed\_0.9} and \textbf{speed\_1.1}: Both achieve 100\% R@1, 0.996 avg similarity (near-perfect). This validates the hypothesis that \textbf{action features provide temporal robustness}. Temporal aggregation (mean, std across frames) normalizes out speed variations, making motion statistics invariant to playback rate within ±10\%.

\textbf{Implication}: StoryHash can detect time-stretched content (TikTok/Reels speed manipulations for music sync) without retraining.

\subsection{Combined Transform Robustness}

\textbf{combined} (crop\_10 + flip\_h + speed\_0.9): 100\% R@1, 0.9903 avg similarity. This multi-perturbation scenario simulates \textbf{WhatsApp forward} (mobile crop, potential mirror, compression). No failures across 89 videos demonstrates \textbf{practical robustness} to real-world content sharing pipelines.

\textbf{Analysis}: Each individual transform is well-handled (crop 100\%, flip\_h 100\%, speed 100\%), and their composition does not introduce emergent failures. This suggests \textbf{additive robustness} rather than multiplicative degradation.

\section{Discussion}

\subsection{Main Finding: Action as Confidence Booster}

Our ablation study reveals a fundamental \textbf{dichotomy} in multi-modal fusion for video retrieval. On the \textbf{detection} dimension (Recall@1), CLIP visual features provide 98.2\% accuracy while action features contribute only +0.4\%, bringing fusion to 98.6\%. This minimal detection improvement suggests visual semantics alone suffice for identifying most matches. However, on the \textbf{confidence} dimension (average similarity), CLIP achieves 0.70 while action features add +0.28, boosting fusion to 0.97—a dramatic 28\% improvement in match certainty.

This dichotomy has profound implications for efficiency-critical systems. Lightweight action features (15D, consuming only 2.8\% of the embedding) justify their computational cost (~0.02s per video for SAM2 tracking) by dramatically improving match confidence without requiring expensive temporal models. For comparison, VideoMAE extracts 768D temporal features at 2.06s inference time per video—51× more dimensions and 100× slower processing. StoryHash's design validates a practical architectural principle: augment strong visual backbones with minimal temporal information to enhance confidence rather than competing on detection capability.

The confidence boost is critical for three real-world deployment scenarios. In \textbf{copyright systems}, high similarity scores (0.97 vs 0.70) provide legal justification for automated DMCA takedowns, reducing false positive risk that could trigger wrongful removals and disputes. For \textbf{content moderation}, borderline matches (similarity 0.65-0.75) require human review to adjudicate potential policy violations; by elevating matches to 0.95+ confidence, action features reduce manual review load and accelerate moderation throughput. Finally, for \textbf{real-time retrieval} at scale, lightweight fusion enables CPU-only deployment: the full StoryHash pipeline (CLIP + action + FAISS search) runs at <0.1s per query on commodity hardware, making it viable for production services handling millions of videos without GPU infrastructure.

\subsection{CLIP Vertical Flip Bias}

The 84.3\% detection rate on flip\_v (vs 100\% on flip\_h) reveals a systematic orientation bias in CLIP's training data. \textbf{Hypothesis}: CLIP's 400M image-text pairs from the internet are overwhelmingly "upright" (cameras held normally). Vertical flips are rare in natural photography, causing the model to learn orientation-dependent features—a manifestation of training distribution bias similar to texture bias in ImageNet-trained models~\cite{geirhos2018imagenettrained}.

\textbf{Evidence from failures}: Videos like \texttt{scooter-black} and \texttt{walking} drop to rank 2-3 after vertical flip despite identical semantic content. The similarity scores (0.88-0.92) remain high but insufficient to beat other videos.

The practical implications of this bias span multiple deployment contexts. On \textbf{social media platforms}, videos accidentally filmed upside-down—rare but possible when phone orientation lock is enabled—may evade detection systems relying on CLIP-based embeddings. While uncommon in organic content, this creates a vulnerability for \textbf{adversarial evasion}: bad actors seeking to circumvent copyright detection could systematically apply vertical flips to pirated content, achieving a 16\% escape rate without perceptual quality loss (unlike compression or cropping which degrade viewing experience).

For system designers, \textbf{mitigation strategies} include fine-tuning CLIP with data augmentation incorporating vertical flips during adaptation to video retrieval tasks. Since CLIP's weights are public and fine-tunable, domain-specific training on video datasets with orientation augmentation could close this bias gap. Alternatively, ensemble approaches combining CLIP with rotation-invariant features (e.g., ViT with learnable positional encodings, or spatial pyramid pooling) might provide complementary robustness.

Our ablation reveals that \textbf{action features cannot compensate} for this CLIP-inherent limitation: flip\_v performance remains identical for fusion (84.3\%) and CLIP-only (84.3\%). This occurs because action features (velocity, direction, spatial spread) are computed in image coordinates and thus also affected by inversion—upside-down motion patterns do not match right-side-up patterns in pixel space. To address flip\_v robustness, solutions must target the visual encoding itself rather than relying on complementary modalities.

\subsection{Rotation as Special Case}

Rotation emerges as the \textbf{only transformation} where action features significantly aid detection: fusion achieves 98.9\% Recall@1 while CLIP-only drops to 96.6\%, a -2.3\% degradation representing 2-3 additional failures out of 89 videos. This unique behavior reveals complementary strengths in visual and temporal modalities.

\textbf{Visual features are sensitive to spatial orientation} because standard convolutional neural networks (and Vision Transformers) encode translation invariance but not rotation invariance. CLIP's ViT-B/32 architecture uses learned positional encodings that capture spatial relationships between patches; when images rotate, these relationships change non-linearly, degrading feature similarity. Objects that appear semantically identical (e.g., a motorcycle at 0\degree vs 10\degree tilt) produce different embeddings because the patch grid alignment shifts.

In contrast, \textbf{motion patterns exhibit rotation invariance} within small angular perturbations (5\degree-10\degree). Velocity and acceleration computed in pixel space remain relatively consistent: an object moving at 50 pixels/frame still moves at approximately 50 pixels/frame after 10\degree rotation, with only minor changes in trajectory angle. Since our action features aggregate statistics (mean, std, max velocity) across all objects and frames, these aggregates are robust to small rotational perturbations that preserve motion magnitude.

This creates \textbf{complementary robustness}: when CLIP's visual discrimination fails due to orientation sensitivity, action features provide a secondary matching signal based on temporal consistency. The 2.3\% improvement is modest but significant—it represents the difference between ranking the correct video at position 1 vs position 2-3 for edge cases with complex rotated scenes.

Future work could explore \textbf{rotation-equivariant architectures} such as Group Convolutional Networks or RotNet-style training to make visual features inherently rotation-invariant, potentially closing this gap and reducing dependence on motion features for oriented perturbations. However, the current result validates multi-modal fusion's value: lightweight temporal features rescue visual failures without architectural complexity.

\subsection{Architecture Efficiency Validation}

Our 527-dimensional embedding achieves near-optimal robustness (98.6\% Recall@1) while maintaining extreme efficiency through deliberate modality proportions. \textbf{CLIP visual features} comprise 512 dimensions (97.2\% of the total embedding), providing the semantic discrimination necessary to separate 89 distinct videos across realistic perturbations. This dominant allocation reflects the empirical finding that visual content carries primary retrieval signal.

\textbf{Action temporal features} occupy only 15 dimensions (2.8\% overhead), yet deliver disproportionate value: +28\% similarity improvement (0.70 → 0.97) with minimal computational cost. Extracting these 15 aggregate statistics from SAM2 object tracking requires approximately 0.02 seconds per video on our M2 Pro hardware—negligible compared to CLIP's 0.15s frame encoding. This asymmetric cost-benefit validates efficiency-focused design: rather than competing with heavyweight temporal models (VideoMAE's 768D features at 2.06s inference), we extract minimal motion statistics that complement visual features without architectural complexity.

\textbf{Scene graph features} contribute 8 dimensions but provide <0.1\% performance impact in current formulation, effectively contributing 0\% to retrieval accuracy. We retain these dimensions in the embedding as a reserved capacity for future work exploring Graph Neural Network encoders or longer video sequences where structural relationships may become more discriminative. For the current evaluation on 2-5 second clips, simple object co-occurrence patterns lack sufficient information to aid retrieval.

The broader architectural lesson is that \textbf{modality contribution is not proportional to dimensionality}. While action features occupy <3\% of the embedding space, they account for the entire confidence improvement over CLIP-only. This finding encourages efficiency-focused multi-modal designs: identify a strong primary modality (CLIP for visual semantics), then augment with lightweight secondary features (action for confidence) rather than pursuing balanced fusion of equally expensive models.

\subsection{Embedding Space Stabilization}

The similarity boost from action features (+0.28) suggests motion acts as an \textbf{embedding space stabilizer}. CLIP embeddings alone have high intra-class variance (0.70 similarity for same video under transforms); adding motion reduces this variance (0.97 similarity), making matches more confident.

\textbf{Geometric interpretation}: Action features may act as a regularizer, pulling transformed versions of the same video closer in embedding space, similar to contrastive learning's positive pair attraction.

\section{Limitations and Future Work}

\subsection{Limitations}

Our work has seven key limitations that constrain generalization and suggest avenues for extension.

\textbf{Small dataset size}: DAVIS 2017 contains only 89 videos, limiting the statistical power of our findings and generalization claims. While this scale enables exhaustive robustness evaluation (1,242 tests across 14 transformations) and reproducible ablation studies, it cannot capture the diversity of real-world video content. \textbf{External validation is critical}: our findings—particularly the action confidence boost (+28\%) and CLIP vertical flip bias (84.3\%)—must be validated on larger-scale datasets with more diverse motion and content (ActivityNet, YouTube-8M) before claiming universal applicability. Large-scale evaluation on YouTube-8M (8 million videos), ActivityNet (20,000 videos), or proprietary platforms (Facebook, TikTok) is needed to establish whether the detection vs confidence dichotomy holds across orders of magnitude more data with greater semantic variety.

\textbf{Short video duration}: Our 2-5 second clips lack long-form narrative structure, temporal scene evolution, or multi-shot composition. Real-world copyright detection targets movies (90-180 minutes), TV episodes (30-60 minutes), and long-form YouTube content (10-60 minutes) where retrieval must handle scene transitions, shot-level alignment, and temporal segmentation. Our findings on action features as confidence boosters may not extend to long videos where temporal structure becomes more discriminative—e.g., narrative arc, pacing, character interactions—requiring hierarchical embeddings or per-shot indexing rather than global frame aggregation.

\textbf{Controlled transformations}: Our ffmpeg-generated perturbations are deterministic and reversible, lacking the complexity of adversarial attacks or real-world evasion techniques. We do not evaluate robustness against deepfakes (face swaps, voice cloning), adversarial patches designed to fool neural networks, sophisticated editing (scene reordering, content insertion), or screen recording artifacts (moiré patterns, refresh rate aliasing). These transformations represent escalating adversarial threats in copyright evasion and misinformation propagation, requiring robustness evaluation beyond standard augmentations.

\textbf{Domain bias in DAVIS}: The dataset comprises natural scenes with salient moving objects (animals, vehicles, sports), biasing evaluation toward content where SAM2 tracking succeeds. Performance on other domains remains unknown: talking heads (minimal motion, face-centric), text-heavy content (presentations, tutorials with slides), abstract visuals (animations, screen recordings of software interfaces), or surveillance footage (static cameras, crowd scenes). Our action features may contribute differently—or not at all—in domains where motion patterns are sparse, predictable, or irrelevant to semantic content.

\textbf{SAM2 tracking sensitivity}: Action features rely entirely on SAM2 tracking quality; failure cases propagate directly into temporal statistics without mitigation. Object tracking failures under severe occlusion, fast motion, or low lighting corrupt computed velocities, accelerations, and interaction densities—transforming informative temporal signals into meaningless noise. Our system lacks failure detection: we do not monitor tracking confidence, exclude unreliable motion statistics, or provide fallback mechanisms when tracking degrades. This creates a critical dependency where SAM2 performance becomes the upper bound for action feature reliability. Robust action feature extraction requires tracking quality assessment (e.g., per-frame confidence thresholds) or fallback to optical flow when object-level tracking fails, ensuring graceful degradation rather than catastrophic error propagation.

\textbf{Scene graph underutilization}: Graph features contribute <0.1\% to retrieval performance in our current formulation, representing wasted computational effort and embedding capacity (8 dimensions). The histogram-based aggregation of spatial and temporal relations is too simplistic to capture meaningful structural patterns in 2-5 second clips. More sophisticated graph encoders—Graph Neural Networks, attention-based message passing, temporal graph convolutions—may extract value from object relationships, especially in longer videos or structured domains (sports plays, social interactions). Until then, these dimensions remain reserved capacity rather than active contributors.

\textbf{No cross-dataset evaluation}: We train (extract embeddings) and evaluate on the same dataset (DAVIS 2017), leaving zero-shot transfer capability untested. A robust video fingerprint should generalize to unseen domains without retraining: detecting copyright violations on content from different sources, moderation systems handling new platforms, archival search across heterogeneous collections. Our CLIP-based visual features likely transfer well due to internet-scale pre-training, but action and graph features depend on SAM2 tracking quality which varies by domain. Cross-dataset experiments (train on DAVIS, test on ActivityNet) would reveal whether our findings on modality contributions hold universally or require domain-specific tuning.

\subsection{Future Work}

Seven research directions emerge from our findings and limitations, offering opportunities to extend StoryHash's capabilities and validate generalization.

\textbf{Lightweight temporal model comparison}: Our action features (15D from SAM2 tracking) provide a confidence boost at minimal cost, but how do they compare to purpose-built efficient video backbones like MoViNet (Mobile Video Networks, 5-10M parameters) or X3D (eXpanded 3D networks with channel expansion)? A systematic comparison quantifying speed-accuracy trade-offs—inference time, memory footprint, detection rate, similarity—would establish whether hand-crafted motion statistics or learned temporal representations offer better efficiency for retrieval tasks. Such analysis informs architectural choices for production systems balancing performance and computational constraints.

\textbf{Video-text alignment for semantic disambiguation}: Incorporating textual metadata (titles, descriptions, captions, user comments) could enhance retrieval precision through multi-modal semantic alignment. Videos with identical visual content but different narrative contexts (e.g., news footage reused in documentaries vs satire) require textual signals for disambiguation. Recent vision-language models (CLIP, ALIGN, CoCa) demonstrate strong text-image grounding; extending this to video retrieval via hierarchical text encoding (title → description → transcript) may improve top-k ranking when visual similarity alone is ambiguous. This direction connects video fingerprinting to broader video-language understanding research.

\textbf{Multi-dataset evaluation for generalization}: Testing on ActivityNet (diverse actions, long clips), YouTube-8M (internet-scale variety), and MSR-VTT (multi-sentence captions) would validate whether our findings—action as confidence booster, CLIP flip bias, rotation compensation—hold across domains, durations, and quality levels. Cross-dataset experiments (train embeddings on DAVIS, test retrieval on ActivityNet) assess zero-shot transfer capability critical for real-world deployment where content sources are heterogeneous. Negative transfer (performance degradation) would reveal domain-specific brittleness requiring adaptation strategies.

\textbf{Real-world perturbation benchmark}: Extending robustness evaluation beyond ffmpeg transformations to adversarial and naturalistic perturbations—screen recordings with moiré patterns, smartphone captures with camera shake, deepfake manipulations, adversarial patches, sophisticated editing (scene reordering, content insertion)—addresses escalating evasion threats. Collaboration with red teams simulating copyright circumvention or misinformation campaigns would identify failure modes invisible in controlled augmentation studies. Such benchmarks inform defensive strategies: ensemble models, certified robustness, adversarial training.

\textbf{Hash collision analysis at scale}: Our 89-video evaluation cannot assess false positive rate—how often do unrelated videos produce high similarity scores? Scaling to 10K-100K videos enables collision analysis: probability that random video pairs exceed similarity threshold, distribution of false match rates by content type, impact of embedding dimensionality on discrimination. High collision rates necessitate collision-resistant hashing (locality-sensitive hashing with multiple tables, learned hashing with pairwise loss) or post-retrieval verification (temporal alignment, frame-level matching) to maintain precision in large databases.

\textbf{Per-shot alignment for long videos}: Extending StoryHash to movies, TV shows, and long-form content requires temporal segmentation into shots or scenes, extracting embeddings per segment, and aligning query-database matches at shot level rather than globally. Hierarchical retrieval—coarse matching of video-level embeddings followed by fine-grained shot alignment—balances efficiency and precision. This approach handles partial matches (detecting 30-second clip within 2-hour movie) and temporal reordering (scenes shuffled during editing), addressing real-world copyright detection where infringing content is often excerpted or remixed.

\textbf{CLIP fine-tuning with orientation augmentation}: Our discovery of CLIP's vertical flip bias (84.3\% detection vs 100\% horizontal) motivates domain adaptation via fine-tuning on video retrieval tasks with aggressive data augmentation including vertical flips, rotations, and perspective warps. Since CLIP's weights are public and fine-tunable, such adaptation could close the 16-percentage-point gap while preserving semantic understanding. Alternatively, training rotation-equivariant visual encoders (Group CNNs, steerable filters) from scratch may provide inherent robustness without augmentation dependence, though at higher computational cost during training.

\section{Conclusion}

We presented \textbf{StoryHash}, a lightweight multi-modal fusion architecture for robust video retrieval, achieving 98.6\% Recall@1 and 0.9748 average similarity across 1,242 tests spanning 14 realistic transformations. Our systematic ablation study reveals a fundamental insight: \textbf{action features boost confidence (+28\% similarity) but minimally impact detection (+0.4\% Recall@1)}, validating efficiency-focused architectures where lightweight temporal features (15D, 2.8\% overhead) dramatically improve match certainty without expensive spatiotemporal models.

We identified a \textbf{CLIP vertical flip bias} (84.3\% detection vs 100\% horizontal) stemming from training data distribution, creating adversarial vulnerability that action features cannot compensate. Conversely, \textbf{rotation is the only transformation} where action features aid detection (-2.3\% degradation without), demonstrating complementary robustness where visual orientation sensitivity is rescued by motion invariance. These findings inform both system design—prioritize strong visual backbones with minimal temporal augmentation—and model improvement—address CLIP's orientation bias through fine-tuning or architecture changes.

Our work provides quantitative evidence for a design principle often assumed but rarely validated: in multi-modal fusion, \textbf{modality contributions are not uniform}. Visual features provide discrimination (98.2\% detection alone), temporal features provide confidence (0.97 similarity vs 0.70), and structural features await better encoders (current <0.1\% contribution). This asymmetry enables practical deployment: StoryHash runs at <0.1s per query on CPU hardware, making it viable for production video retrieval at scale—copyright detection resisting evasion, content moderation with high certainty, and archival search with lightweight infrastructure.

Future work must address limitations—small dataset, short videos, controlled perturbations—through large-scale evaluation, long-form extension, and adversarial benchmarking. Yet our core finding remains actionable: when building video retrieval systems, invest in visual semantics for detection, augment with minimal motion for confidence, and measure contributions separately to avoid architectural complexity without empirical justification.
